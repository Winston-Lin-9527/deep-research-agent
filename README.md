# Deep Research Agent

This repository contains a small multi-agent research assistant demo with a Python backend (LangGraph + LangChain + Google Generative AI) and a React + Vite frontend. The app exposes a chat UI where users can toggle a "Deep Research" mode which switches the assistant between `deep_research_agent` and `simple_chat`.

This README covers how to set up and run the project locally, environment variables, and troubleshooting tips. This README is generated by AI agent :)

## Repository layout

- `backend/` - Python backend and LangGraph graphs
	- `src/` - Python source files (agents, app, utils)
	- `langgraph.json` - graph entrypoints and config
	- `pyproject.toml` - backend Python project config
- `frontend/` - React + Vite app
	- `src/` - React components

## Requirements

- Python 3.11
- Node.js (16+ recommended)
- pnpm/npm/yarn for frontend (project uses npm)

## Ports

- Backend (LangGraph dev server): `2024` (default in the frontend). If this port is in use, either stop the conflicting process or change the `apiUrl` in `frontend/src/components/ChatApp.tsx`.

## Backend - Quick start

1. Create and activate a Python 3.11 virtual environment.

```bash
python -m venv .venv
source .venv/bin/activate
```

2. Install backend dependencies (use `pip` with `pyproject.toml` or `requirements.txt` if provided).

```bash
# from backend/
pip install -e .
```

3. Configure environment variables. The repo references an `.env` file via `langgraph.json`. Create `backend/.env` with keys required by your LLM provider (for example for Google Generative AI or OpenAI). Example:

```env
# backend/.env
GOOGLE_API_KEY=your_key_here
# other provider keys as needed
```

4. Run the LangGraph app (project uses the `app` entry in `langgraph.json`).

```bash
# from backend/
python -m src.app
```

If your project uses a different runner or a `langgraph` CLI, follow your usual workflow.

## Frontend - Quick start

1. Install dependencies and run the dev server:

```bash
cd frontend
npm install
npm run dev
```

2. The frontend expects the backend at `http://localhost:2024` (see `frontend/src/components/ChatApp.tsx`). If you changed the backend port, update the `apiUrl` in that file.

## Deep Research toggle

The chat UI includes a small switch labeled "Deep Research" next to the text input. When enabled the frontend sets the assistant ID to `deep_research_agent`, otherwise it uses `simple_chat`. This allows quick switching between a more capable research assistant and a simpler chat mode.

Note: Toggling the switch updates the `assistantId` used by the `useStream` hook in the frontend. If you experience caching or stream issues after toggling, reload the page or restart the frontend.

## Troubleshooting

- Port conflicts on 2024: find and stop the process using the port:

```bash
sudo lsof -i :2024
sudo kill <PID>
```

- Google Generative AI 500 errors: transient upstream errors can be retried. If persistent, check your API key, quota, and the developers troubleshooting guide: https://developers.generativeai.google/guide/troubleshooting

- Frontend not connecting: verify `apiUrl` in `frontend/src/components/ChatApp.tsx` and that the backend is running and reachable.

## Development notes

- The LangGraph graphs are configured in `backend/langgraph.json`. Graph entrypoints include:
	- `deep_research_agent` -> `src/full_agent.py:deep_research_agent`
	- `supervisor_agent` -> `src/multi_agent_supervisor.py:supervisor_agent`
	- `researcher_agent` -> `src/research_agent.py:research_agent`
	- `scope_research_agent` -> `src/scoping_agent.py:scope_research`

- The frontend is a small React app using Material UI. The chat input component is at `frontend/src/components/ChatInput.tsx` and the main app is `frontend/src/components/ChatApp.tsx`.
